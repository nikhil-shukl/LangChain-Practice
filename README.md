ğŸš€ Generative AI & RAG with LangChain 1.x

This repository contains end-to-end implementations of Generative AI and Retrieval-Augmented Generation (RAG) systems built using LangChain 1.x, following modern runnable-based pipelines instead of deprecated chain abstractions.

The project demonstrates the complete workflow â€” from data ingestion to vector storage, retrieval, and context-aware LLM generation, using both OpenAI and open-source models (Ollama, HuggingFace).

ğŸ§  Key Concepts Covered

LangChain 1.x runnable pipelines (no deprecated chains)

Retrieval-Augmented Generation (RAG)

Context-aware LLM responses

Vector similarity search

Production-style modular design

ğŸ› ï¸ Tech Stack

LangChain 1.x

OpenAI (GPT-4 / GPT-4o-mini)

Ollama (local LLMs & embeddings)

FAISS & ChromaDB (Vector Stores)

HuggingFace Embeddings

Python

Streamlit (Chat UI)

LangSmith (Tracing & Observability)

ğŸ“‚ Project Structure
llm_projects/
â”‚
â”œâ”€â”€ 1-openai/
â”‚   â”œâ”€â”€ Getting started with LangChain & OpenAI
â”‚   â””â”€â”€ Simple GenAI / RAG apps
â”‚
â”œâ”€â”€ 2-ollama/
â”‚   â”œâ”€â”€ Local LLM usage with Ollama
â”‚
â”œâ”€â”€ 3-DataIngestion/
â”‚   â”œâ”€â”€ Web, text, JSON, Arxiv loaders
â”‚
â”œâ”€â”€ 4-Embeddings/
â”‚   â”œâ”€â”€ OpenAI embeddings
â”‚   â”œâ”€â”€ Ollama embeddings
â”‚   â””â”€â”€ HuggingFace embeddings
â”‚
â”œâ”€â”€ 5-VectorStore/
â”‚   â”œâ”€â”€ FAISS
â”‚   â”œâ”€â”€ ChromaDB
â”‚   â””â”€â”€ Similarity search & retrievers
â”‚
â”œâ”€â”€ Streamlit Chat App
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ” RAG Architecture (LangChain 1.x)
User Query
   â†“
Retriever (FAISS / Chroma)
   â†“
Relevant Documents
   â†“
Prompt Template
   â†“
LLM (OpenAI / Ollama)
   â†“
StrOutputParser
   â†“
Final Answer

âœ¨ Highlights

âœ… Migrated from deprecated chains to pure runnable pipelines

âœ… Explicit retrieval and context injection (no black-box logic)

âœ… Supports both cloud and local LLMs

âœ… Clean, debuggable, production-ready architecture

âœ… Streamlit-based chatbot interface

ğŸ§ª Example: Runnable-based RAG (LangChain 1.x)
docs = retriever.invoke(question)
context = "\n".join([doc.page_content for doc in docs])

answer = rag_pipeline.invoke({
    "context": context,
    "question": question
})

ğŸš€ Getting Started

Clone the repository

Create a virtual environment

Install dependencies:

pip install -r requirements.txt


Add your API keys in .env

Run notebooks or Streamlit app

ğŸ¯ Learning Outcome

This repository is ideal for:

Developers learning modern LangChain 1.x

Understanding real-world RAG pipelines

Building production-grade GenAI applications

ğŸ“Œ Author

Nikhil Shukla
Generative AI | LangChain | RAG | LLM Engineering
